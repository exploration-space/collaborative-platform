{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROVIDEDH Collaborative platform\n",
    "## Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from functools import reduce\n",
    "from lxml import etree as et\n",
    "from lxml.etree import Element\n",
    "import itertools\n",
    "import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "sp_nlp_en = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespaces = {'tei': 'http://www.tei-c.org/ns/1.0', 'xml': 'http://www.w3.org/XML/1998/namespace'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body_text_nodes(tree_root, namespaces={'tei': 'http://www.tei-c.org/ns/1.0', 'xml': 'http://www.w3.org/XML/1998/namespace'}):\n",
    "    \"\"\"Retrieve all text nodes in the body of lxml tree.\"\"\"\n",
    "\n",
    "    body = tree_root.find('.//tei:body', namespaces=namespaces)\n",
    "    return body.xpath('.//text()')\n",
    "\n",
    "def filter_out_empty_text_nodes(text_nodes):\n",
    "    not_empty = lambda node: node.strip() != ''\n",
    "    return filter(not_empty, text_nodes)\n",
    "\n",
    "def remove_extra_spaces(text_nodes):\n",
    "    inner_spaces = re.compile('[ \\n\\r]{2,}')\n",
    "    tree_root = None\n",
    "\n",
    "    for node in text_nodes:\n",
    "        if tree_root is None:\n",
    "            tree_root = node.getparent().getroottree()\n",
    "\n",
    "        trimmed_text= inner_spaces.sub(' ', node).strip()\n",
    "\n",
    "        if node.is_text:\n",
    "            node.getparent().text = trimmed_text\n",
    "        elif node.is_tail:\n",
    "            node.getparent().tail = trimmed_text\n",
    "\n",
    "    if tree_root is not None:\n",
    "        text_nodes = get_body_text_nodes(tree_root)\n",
    "      \n",
    "    return text_nodes\n",
    "\n",
    "def assign_types(text_nodes):\n",
    "    def _get_ancestors(node):\n",
    "        parent = node.getparent()\n",
    "        parent_ancestors = tuple(parent.iterancestors())\n",
    "\n",
    "        ancestors = (parent,)+parent_ancestors if node.is_text else parent_ancestors\n",
    "        return ancestors\n",
    "\n",
    "    namespace_regex = re.compile('\\{.*\\}')\n",
    "    remove_namespace = lambda namespace: namespace_regex.sub('', namespace)\n",
    "\n",
    "    get_types = lambda node: tuple(remove_namespace(anc.tag) for anc in _get_ancestors(node))\n",
    "\n",
    "    return ((node, get_types(node)) for node in text_nodes)\n",
    "\n",
    "def remove_irrelevant_tags(text_nodes, irrelevant_tags):\n",
    "    return ((node, tuple(tag for tag in tags if tag not in irrelevant_tags)) for node,tags in text_nodes)\n",
    "\n",
    "def remove_irrelevant_tei_tags(text_nodes): \n",
    "    return remove_irrelevant_tags(text_nodes, \n",
    "            ('unclear', 'TEI', 'body', 'text', 'damage', 'add', 'supplied', 'div', 'span', 'p', 'del', 'note'))\n",
    "\n",
    "def assign_entity_2_fragments(text_nodes, tei_2_entity):\n",
    "    data = []\n",
    "    for fragment in text_nodes:\n",
    "        tags = fragment[1]\n",
    "        entity = 'text'\n",
    "        for tag in tags:\n",
    "            if tag in tei_2_entity:\n",
    "                entity = tei_2_entity[tag]\n",
    "                break\n",
    "    \n",
    "        data.append((fragment[0], entity))\n",
    "\n",
    "    return data\n",
    "\n",
    "def apply_tokenizer(text_nodes, tokenizer):\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for fragment in text_nodes:\n",
    "        fragment_tokens = tokenizer(fragment[0])\n",
    "        tokens.extend(fragment_tokens)\n",
    "        tags.extend(tuple(fragment[1] for _ in range(len(fragment_tokens))))\n",
    "\n",
    "    return tokens, tags\n",
    "\n",
    "def apply_str_tokenizer(text_nodes):\n",
    "    return apply_tokenizer(text_nodes, str)\n",
    "\n",
    "def filter_annotated(text_nodes):\n",
    "        return filter(lambda text_node: len(text_node[1]) == 0, text_nodes)\n",
    "\n",
    "def process(xml_tree):\n",
    "    processing = (\n",
    "        get_body_text_nodes, \n",
    "        remove_extra_spaces,\n",
    "        filter_out_empty_text_nodes,\n",
    "        assign_types,\n",
    "        remove_irrelevant_tei_tags,\n",
    "        #apply_str_tokenizer,\n",
    "        tuple\n",
    "    )\n",
    "\n",
    "    return reduce(lambda x,f: f(x), processing, xml_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gazeette():\n",
    "    processed_depositions = os.listdir('./depositions_subset/processed')\n",
    "    processed = ()\n",
    "    for dep_name in processed_depositions:\n",
    "        with open(os.path.join('./depositions_subset/processed', dep_name)) as f:\n",
    "            dep_raw = f.read()\n",
    "            dep_tree = et.fromstring(dep_raw.encode())\n",
    "            processed += process(dep_tree)\n",
    "    zipped = filter(lambda node: len(node[1]) > 0, processed)\n",
    "    gazeette = {text:tag for text, tag in ((x[0], x[1][0]) for x in zipped)}\n",
    "    return gazeette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('towne of Ballelahartye', 'placeName'),\n",
       " ('Katherine mc Gilleigh', 'person'),\n",
       " ('Catherine mc Gilleigh', 'person'),\n",
       " ('Patricke mc Gilliegh', 'person'),\n",
       " ('town of Ballelaharty', 'placeName'),\n",
       " ('William Wolferston', 'person'),\n",
       " ('County of Monaghan', 'placeName'),\n",
       " ('Cormacke mc Guyre', 'person'),\n",
       " ('William Hitchcock', 'name'),\n",
       " ('Patrick O Dorogan', 'person'),\n",
       " ('Patrick mc Enalye', 'person'),\n",
       " ('Saterday morninge', 'date'),\n",
       " ('first of december', 'date'),\n",
       " ('County of Ardmagh', 'placeName'),\n",
       " ('Kath: mc Gillegh', 'person'),\n",
       " ('Kath mc Gilleigh', 'person'),\n",
       " ('north of Ireland', 'placeName'),\n",
       " ('Lawrence Beddell', 'person'),\n",
       " ('Cittie of Dublin', 'placeName'),\n",
       " ('County of Armagh', 'placeName'),\n",
       " ('George Blundell', 'name'),\n",
       " ('County of Louth', 'placeName'),\n",
       " ('Laurence Bedlow', 'person'),\n",
       " ('Sir Same Mayart', 'person'),\n",
       " ('Citty of Dublin', 'placeName'),\n",
       " ('Will: Hitchcock', 'person'),\n",
       " ('Jur 7 febr 1641', 'date'),\n",
       " ('William Aldrich', 'name'),\n",
       " ('Mathew Talbott', 'person'),\n",
       " ('towne of Louth', 'placeName'),\n",
       " ('Rory mc Enuske', 'person'),\n",
       " ('Giles Dewhurst', 'person'),\n",
       " ('City of Dublin', 'placeName'),\n",
       " ('Henry Brereton', 'name'),\n",
       " ('Captain Wooll', 'person'),\n",
       " ('Banabye Toole', 'person'),\n",
       " ('Ballelahartye', 'placeName'),\n",
       " ('Carbery Birne', 'person'),\n",
       " ('Samuel Mayart', 'name'),\n",
       " ('6 of Nov 1641', 'date'),\n",
       " ('Carbery Byrne', 'person'),\n",
       " ('7o ffebr 1641', 'date'),\n",
       " ('Hen: Brereton', 'person'),\n",
       " ('John. Harding', 'person'),\n",
       " ('09 June 1653', 'date'),\n",
       " ('James Traill', 'name'),\n",
       " ('Captain Wool', 'person'),\n",
       " ('Murferstowne', 'placeName'),\n",
       " ('three weekes', 'time'),\n",
       " ('Ballelaharty', 'placeName'),\n",
       " ('Hue mc Doine', 'person'),\n",
       " ('lord Mcguire', 'person'),\n",
       " ('John Harding', 'person'),\n",
       " ('7th of march', 'date'),\n",
       " ('John Sterne', 'name'),\n",
       " ('Ballehegert', 'placeName'),\n",
       " ('Hugh Cressy', 'name'),\n",
       " ('Hue mc Done', 'person'),\n",
       " ('Same Mayart', 'person'),\n",
       " ('John Watson', 'name'),\n",
       " ('Ja: Traill', 'name'),\n",
       " ('G Blundell', 'person'),\n",
       " ('Alice Hogg', 'person'),\n",
       " ('Luke Toole', 'person'),\n",
       " ('Sa: Mayart', 'person'),\n",
       " ('Capt Hecle', 'person'),\n",
       " ('O dorogans', 'person'),\n",
       " ('Liverpoole', 'placeName'),\n",
       " ('Joh Watson', 'person'),\n",
       " ('ffebr 1641', 'date'),\n",
       " ('Ja Traill', 'name'),\n",
       " ('Hu Cressy', 'person'),\n",
       " ('Wedensday', 'date'),\n",
       " ('yorkshire', 'placeName'),\n",
       " ('thursday', 'placeName'),\n",
       " ('saterday', 'date'),\n",
       " ('Saterday', 'date'),\n",
       " ('McMahons', 'organization'),\n",
       " ('mcguires', 'organization'),\n",
       " ('Mcmahons', 'organization'),\n",
       " ('ebr 1641', 'date'),\n",
       " ('Patrick', 'person'),\n",
       " ('October', 'date'),\n",
       " ('Kilcock', 'placeName'),\n",
       " ('tuseday', 'date'),\n",
       " ('Ireland', 'placeName'),\n",
       " ('Beddell', 'person'),\n",
       " ('England', 'placeName'),\n",
       " ('William', 'name'),\n",
       " ('Aldrich', 'person'),\n",
       " ('7 march', 'date'),\n",
       " ('sunday', 'date'),\n",
       " ('Dublin', 'placeName'),\n",
       " ('friday', 'date'),\n",
       " ('munday', 'date'),\n",
       " ('Clonee', 'placeName'),\n",
       " ('monday', 'date'),\n",
       " ('Louth', 'placeName'),\n",
       " ('mark', 'supplied'),\n",
       " ('1641', 'date'),\n",
       " ('75', 'supplied')]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gazeette = create_gazeette()\n",
    "sorted(gazeette.items(), key=lambda a: -len(a[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Text Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    of Dublin Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    of <place>Dublin</place> Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "(' of ', ' Doctor in Divinity being duely\\n')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    He,\\n    <person>\\n    Alex\\n    </person>\\n    of Dublin Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    He,\\n    <person>\\n    Alex\\n    </person>\\n    of <place>Dublin</place> Doctor in Divinity being duely\\n</p>\\n</span>'\n",
      "(' of ', ' Doctor in Divinity being duely\\n')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    of Dublin Doctor\\n    <person>\\n    Alex\\n    </person>\\n    in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    of <place>Dublin</place> Doctor\\n    <person>\\n    Alex\\n    </person>\\n    in Divinity being duely\\n</p>\\n</span>'\n",
      "('\\n    of ', ' Doctor\\n    ')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    from\\n    <person>\\n    Alex\\n    </person>\\n    Dublin <date>Doctor</date> in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    from\\n    <person>\\n    Alex\\n    </person>\\n    <place>Dublin</place> <date>Doctor</date> in Divinity being duely\\n</p>\\n</span>'\n",
      "(' of ', ' ')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    Doctor in Divinity being of Dublin\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex\\n    </person>\\n    Doctor in Divinity being of <place>Dublin</place>\\n</p>\\n</span>'\n",
      "(' of ', '\\n')\n",
      "\n",
      "-----------------------\n",
      "\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex from Dublin\\n    </person>\\n    of in Divinity being duely\\n</p>\\n</span>'\n",
      "b'<span>\\n<p>\\n    <person>\\n    Alex from <place>Dublin</place>\\n    </person>\\n    of in Divinity being duely\\n</p>\\n</span>'\n",
      "('\\n    Alex from ', '\\n    ')\n"
     ]
    }
   ],
   "source": [
    "a = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    of Dublin Doctor in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "a_ = a.xpath('//text()')\n",
    "\n",
    "b = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    He,\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    of Dublin Doctor in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "b_ = b.xpath('//text()')\n",
    "\n",
    "c = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    of Dublin Doctor\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "c_ = c.xpath('//text()')\n",
    "\n",
    "d = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    from\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    Dublin <date>Doctor</date> in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "d_ = d.xpath('//text()')\n",
    "\n",
    "e = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    <person>\n",
    "    Alex\n",
    "    </person>\n",
    "    Doctor in Divinity being of Dublin\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "e_ = e.xpath('//text()')\n",
    "\n",
    "f = et.fromstring('''\n",
    "<span>\n",
    "<p>\n",
    "    <person>\n",
    "    Alex from Dublin\n",
    "    </person>\n",
    "    of in Divinity being duely\n",
    "</p>\n",
    "</span>\n",
    "''')\n",
    "f_ = f.xpath('//text()')\n",
    "\n",
    "print(et.tostring(a))\n",
    "leafs = wrap_text_in_tag(a_[3], 'Dublin', 'place')\n",
    "print(et.tostring(a))\n",
    "print(leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(b))\n",
    "leafs = wrap_text_in_tag(b_[3], 'Dublin', 'place')\n",
    "print(et.tostring(b))\n",
    "print(leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(c))\n",
    "leafs = wrap_text_in_tag(c_[1], 'Dublin', 'place')\n",
    "print(et.tostring(c))\n",
    "print(leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(d))\n",
    "leafs = wrap_text_in_tag(d_[3], 'Dublin', 'place')\n",
    "print(et.tostring(d))\n",
    "print(leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(e))\n",
    "leafs = wrap_text_in_tag(e_[3], 'Dublin', 'place')\n",
    "print(et.tostring(e))\n",
    "print(leafs)\n",
    "\n",
    "print('\\n-----------------------\\n')\n",
    "\n",
    "print(et.tostring(f))\n",
    "leafs = wrap_text_in_tag(f_[2], 'Dublin', 'place')\n",
    "print(et.tostring(f))\n",
    "print(leafs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text_in_tag(text,substring,tag_name):\n",
    "    text_node = text.getparent()\n",
    "    parent = text_node.getparent()\n",
    "    unprocessed_leaf_nodes: tuple\n",
    "    partitions = text.partition(substring)\n",
    "    \n",
    "    new_element = et.Element(tag_name)\n",
    "    new_element.text = partitions[1]\n",
    "    new_element.tail = partitions[2]\n",
    "    \n",
    "    new_element_tail = new_element.xpath(\"//text()\")[1]\n",
    "        \n",
    "    if text.is_text:\n",
    "        text_node.text = partitions[0]\n",
    "        text_node.insert(0, new_element)\n",
    "        \n",
    "        parent_text = text_node.xpath('.//text()')[0]      \n",
    "        unprocessed_leaf_nodes = (parent_text, new_element_tail)\n",
    "        \n",
    "    elif text.is_tail:\n",
    "        index = parent.index(text_node) + 1\n",
    "        text_node.tail = partitions[0]\n",
    "        parent.insert(index,new_element)\n",
    "        \n",
    "        parent_tail = None\n",
    "        for x in p.getparent().xpath('.//text()'):\n",
    "            if x.getparent() == p and x.is_tail == True:\n",
    "                parent_tail = x\n",
    "        unprocessed_leaf_nodes = (parent_tail, new_element_tail)\n",
    "        \n",
    "    return unprocessed_leaf_nodes\n",
    "\n",
    "def annotate(dep_folder, dep_name, gazeette):\n",
    "    with open(os.path.join(dep_folder, dep_name)) as f:\n",
    "        dep_raw = f.read()\n",
    "    dep_tree = et.fromstring(dep_raw.encode())\n",
    "        \n",
    "    dep_annotated = apply_gazeette( apply_rules(dep_tree), gazeette)\n",
    "    dep_post_annotated = apply_post_annotation(dep_annotated)\n",
    "    \n",
    "    return dep_annotated\n",
    "\n",
    "def apply_rules(dep_tree):\n",
    "    namespaces = {'tei': 'http://www.tei-c.org/ns/1.0', 'xml': 'http://www.w3.org/XML/1998/namespace'}\n",
    "    months = [\n",
    "        'january', 'jan', 'february', 'feb', 'march', \n",
    "        'mar', 'april', 'jpr', 'may', 'may', 'june', \n",
    "        'jun', 'july', 'jul', 'august', 'aug', \n",
    "        'september', 'sep', 'october', 'oct', 'november', \n",
    "        'nov', 'december', 'dec'\n",
    "    ]\n",
    "    \n",
    "    processing = (\n",
    "        get_body_text_nodes, \n",
    "        filter_out_empty_text_nodes,\n",
    "        assign_types,\n",
    "        remove_irrelevant_tei_tags,\n",
    "        filter_annotated,\n",
    "        lambda text_nodes: map(lambda node: node[0], text_nodes),\n",
    "        list\n",
    "    )\n",
    "    \n",
    "    process = lambda xml_tree: reduce(lambda x,f: f(x), processing, xml_tree)\n",
    "    \n",
    "    dep_text_nodes = process(dep_tree)\n",
    "                                      \n",
    "    while len(dep_text_nodes) > 0:\n",
    "        node = dep_text_nodes.pop(0)\n",
    "        words = node.split(' ')\n",
    "             \n",
    "        if len(words) < 3:\n",
    "            pass\n",
    "                                      \n",
    "        text_len = 3 # county of x, barony of x\n",
    "                                      \n",
    "        match = False\n",
    "        for i in range(0, len(words) + 1 - text_len):\n",
    "            matching_text = ' '.join(words[i : i+text_len]).strip()\n",
    "                \n",
    "            if matching_text.lower().startswith('county of'):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'placeName')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "            elif matching_text.lower().startswith('town of'):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'placeName')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "            elif matching_text.lower().startswith('province of'):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'placeName')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "            elif matching_text.lower().startswith('countie of'):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'placeName')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "            elif matching_text.lower().startswith('towne of'):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'placeName')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "            elif matching_text.lower().startswith('barony of'):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'placeName')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "            elif words[i+1].lower() in months:\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'date')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "                                          \n",
    "    return dep_tree\n",
    "\n",
    "def apply_post_annotation(dep_tree): \n",
    "    processing = (\n",
    "        get_body_text_nodes, \n",
    "        filter_out_empty_text_nodes,\n",
    "        assign_types,\n",
    "        remove_irrelevant_tei_tags,\n",
    "        filter_annotated,\n",
    "        lambda text_nodes: map(lambda node: node[0], text_nodes),\n",
    "        list\n",
    "    )\n",
    "    \n",
    "    process = lambda xml_tree: reduce(lambda x,f: f(x), processing, xml_tree)\n",
    "    dep_text_nodes = process(dep_tree)\n",
    "                                      \n",
    "    while len(dep_text_nodes) > 0:\n",
    "        node = dep_text_nodes.pop(0)\n",
    "        words = node.split(' ')\n",
    "             \n",
    "        if len(words) < 4:\n",
    "            pass\n",
    "                                      \n",
    "        text_len = 4 # dates\n",
    "                                      \n",
    "        match = False\n",
    "        for i in range(0, len(words) + 1 - text_len):\n",
    "            matching_text = ' '.join(words[i : i+text_len]).strip()\n",
    "                \n",
    "            if re.match('[0-9]{4}', words[i+1].lower()):\n",
    "                left_nodes = wrap_text_in_tag(node, matching_text, 'date')\n",
    "                dep_text_nodes.insert(0, left_nodes[1])\n",
    "                dep_text_nodes.insert(0, left_nodes[0])\n",
    "                match = True\n",
    "                break\n",
    "                                          \n",
    "    return dep_tree\n",
    "    \n",
    "def apply_gazeette(dep_tree, gazeette):\n",
    "    namespaces = {'tei': 'http://www.tei-c.org/ns/1.0', 'xml': 'http://www.w3.org/XML/1998/namespace'}\n",
    "    \n",
    "    processing = (\n",
    "        get_body_text_nodes, \n",
    "        filter_out_empty_text_nodes,\n",
    "        assign_types,\n",
    "        remove_irrelevant_tei_tags,\n",
    "        filter_annotated,\n",
    "        lambda text_nodes: map(lambda node: node[0], text_nodes),\n",
    "        list\n",
    "    )\n",
    "    \n",
    "    process = lambda xml_tree: reduce(lambda x,f: f(x), processing, xml_tree)\n",
    "    \n",
    "    dep_text_nodes = process(dep_tree)\n",
    "    \n",
    "    gazeette_sorted = sorted(gazeette.items(), key=lambda a: -len(a[0]))\n",
    "    while len(dep_text_nodes) > 0:\n",
    "        node = dep_text_nodes.pop(0)\n",
    "        words = node.split(' ')\n",
    "                \n",
    "        for entry in gazeette_sorted:\n",
    "            text_len = len(entry[0].split(' '))\n",
    "                \n",
    "            if len(words) < text_len:\n",
    "                pass\n",
    "            \n",
    "            match = False\n",
    "            for i in range(0, len(words) + 1 - text_len):\n",
    "                if i >= len(words) or (i + text_len) >= len(words):\n",
    "                    break\n",
    "                matching_text = words[i].strip() if text_len == 1 else ' '.join(words[i : i+text_len]).strip()\n",
    "                \n",
    "                if entry[0] in matching_text:\n",
    "                    left_nodes = wrap_text_in_tag(node, entry[0], entry[1])\n",
    "                    dep_text_nodes.insert(0, left_nodes[1])\n",
    "                    dep_text_nodes.insert(0, left_nodes[0])\n",
    "                    match = True\n",
    "                    break\n",
    "                    \n",
    "            if match:\n",
    "                break\n",
    "                    \n",
    "    return dep_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Apply Gazeette to Depositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_folder = './depositions_subset/to_process'\n",
    "depositions = os.listdir(dep_folder)\n",
    "processed = './depositions_subset/to_process/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 196/197 [02:04<00:00,  1.57it/s]\n",
      "100%|██████████| 197/197 [00:35<00:00,  5.56it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.tqdm._instances.clear()\n",
    "\n",
    "gazeette = create_gazeette()\n",
    "\n",
    "for dep_name in tqdm.tqdm(depositions):\n",
    "    #print('\\n')\n",
    "    #print(dep_name)\n",
    "\n",
    "    try:\n",
    "        if Path(os.path.join(dep_folder, dep_name)).is_file():\n",
    "            applied = annotate(dep_folder, dep_name, gazeette)\n",
    "            with open(os.path.join(processed, dep_name), 'w') as f:\n",
    "                f.write(et.tostring(applied).decode('UTF-8'))\n",
    "    except AttributeError as e:\n",
    "        pass #print('!! Is empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12:20 el último revisado manualmente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
